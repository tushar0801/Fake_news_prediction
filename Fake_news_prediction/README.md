# Fake_news_prediction
in this we will train our model on fake and real news so that it can differentiate between fake and real news in real time 
First, let's go over the libraries being imported:  numpy is a library for numerical computing in Python. pandas is a library for data manipulation and analysis in Python. matplotlib.pyplot and seaborn are both visualization libraries for Python.
nltk is a library for natural language processing in Python. 
re is a library for regular expressions in Python.
WordCloud is a library for generating word clouds in Python.
tensorflow.keras is a deep learning library for building and training models in Python. 
train_test_split and classification_report are functions from the scikit-learn library, which is a machine learning library for Python. 
The first few lines of code load in two CSV files (Fake.csv and True.csv) using the read_csv function from Pandas. df1 and df2 are the data frames containing the data from these files. The info and describe functions are then called on df1 to get a summary of its contents, and a countplot is generated using sns.countplot.  Next, the code generates word clouds for the text column in df1 and df2 using the WordCloud library. A list of unknown publishers is then created by iterating over the rows in df2 and checking for certain conditions. 
Rows in df2 that have unknown publishers are added to temp_text, and the corresponding publisher is set to 'unknown'. The final result is stored in df2 with a new column for the publisher.  The text column in df1 and df2 is then concatenated with the title column, and a new class column is added to both data frames. df1 and df2 are then concatenated into a single data frame called data. 
The code then installs several libraries using pip, including spacy, beautifulsoup4, textblob, and preprocess_kgptalkie. The latter library is imported and used to remove special characters from the text column in data.  The next several lines of code involve training a Word2Vec model on the text data in data. This model is used to generate an embedding matrix, which is used to initialize the weights of an embedding layer in a neural network. The neural network consists of an embedding layer, an LSTM layer with 128 units, and a dense output layer with a sigmoid activation function. 
The model is then compiled with an optimizer of Adam, a binary cross-entropy loss function, and an accuracy metric.  The data is split into training and testing sets using the train_test_split function from scikit-learn. The model is then trained on the training set and validated on a portion of the training set using a validation split of 0.3 and 2 epochs.  Finally, the model is used to make predictions on the test set, and the classification_report function from scikit-learn is used to generate a report on the classification results. The accuracy_score function is also used to calculate the accuracy of the model.
